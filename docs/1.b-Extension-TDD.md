# Player Management Service Extension - Technical Design Document

## 1. Goals

- **Extend the Player Management Service (PMS)** to support **server-authoritative real-time multiplayer** gameplay for a small to moderate concurrency RPG (inspired by Baldur’s Gate).
- **Tighten security and data integrity**:
  - Authentication and session validation on every request.
  - Enforce character ownership.
  - Clients send **intents**, not final data.
- **Achieve low latency at moderate concurrency**:
  - Handle thousands of concurrent users efficiently.
  - Fast access to player data through in-memory caching.
  - Avoid costly synchronous database access per request.
- **Design for Simplicity and Evolvability**:
  - Favor a simple, robust architecture first.
  - Plan clear exit paths for scaling and resilience improvements later.

## 2. Non-Goals

- **Not targeting millions of concurrent players**: Support small to mid-size multiplayer sessions (hundreds to low-thousands of CCU).
- **Not implementing Game Simulation services**: Only **Player and Character Management** is covered.
- **Not covering live session management or matchmaking**: No real-time combat simulation or session orchestration.

## 3. Proposed Architectural Changes

| Area                   | Current                          | Proposed Extension                                    |
|------------------------|----------------------------------|------------------------------------------------------|
| Deployment             | Single server                    | Horizontally scalable, stateful service              |
| Data Access            | Direct DB writes                 | In-memory cache per instance + async DB writes       |
| Authentication         | Basic API key                    | Token-based session authentication (JWT/Bearer)      |
| Ownership Enforcement  | None                             | Every request validated against session ownership    |
| Concurrency Handling   | None                             | Optimistic concurrency (versioning)                  |
| Consistency Guarantees | Best-effort                      | Server authoritative (client sends intent, not result) |
| API Gateway            | None                             | Centralized routing, authentication, sharding       |

### New Components
- **API Gateway + Shard Router**: Route requests based on userId.
- **Auth Service**: Issue and validate session tokens.
- **Player Management Service**:
  - **Local in-memory cache** for active character data.
  - **Lazy loading** from DB on cache miss.
  - **Optimistic concurrency control** (to achieve performance under concurrency).
- **Monitoring Stack**: Prometheus + Grafana + OpenTelemetry.

### Deployment Diagrams

#### Login and Logout Flow

![Authentication Flow](1.b-auth.png)

#### Authenticated Calls

![Deployment Diagram](1.b-deployment.png)

## 4. Simplicity and Exit Plan

| Challenge                        | Initial Simple Solution                           | Migration / Exit Path        |
|----------------------------------|---------------------------------------------------|-------------------------------|
| Low-latency player data access   | Local **in-memory cache** per server              | Add Redis/Distributed Cache later if needed |
| Resilience to node failure       | **Lazy reload** character state from DB on cache miss | Optional cache replication in Redis |
| Load distribution                | **Shard routing** at API Gateway based on `userId` | Move to Consistent Hashing |
| Scaling players across servers   | **Horizontal scaling** (stateless per group of players) | Introduce service discovery |

**Key principle**:
- **No external cache** dependency at MVP.
- **Accept brief reloading cost** if a server goes down.
- **Smooth migration** to more complex caching or clustering only if needed.


> **Tradeoff**: We favor a simple in-memory design initially to optimize for developer velocity and operational simplicity. If player CCU increases or failover speed becomes critical, we can incrementally add Redis or distributed caching without redesigning core service behavior.

## 5. Shard Routing Strategy

* Requests are routed to Player Management Service instances based on userId extracted from the session token.

* `characterId` is always passed along with the request and validated inside the PMS to ensure ownership.

* This allows grouping all of a user's characters on the same shard, simplifying cache locality and minimizing cross-shard communication.

## 5. Proposed API Changes and Functionality

All API endpoints except for session login (`POST /session/login`) require a valid Bearer token for authentication.

- **`POST /session/login`**: **(New)** Authenticate a player and return a session token (Bearer token).
- **`POST /session/logout`**: **(New)** Revoke the session token and clear session-related cache.
- **`POST /characters`**: Create a new character. Requires Bearer token; links character to session user ID.
- **`GET /characters`**: **(New)** Fetch all characters associated with the current session user ID.
- **`GET /characters/{id}`**: Fetch the latest authoritative character state, including derived stats, from cache (fallback to database).
- **Other Character Modifications (e.g., PATCH /characters/{id}/stats/hitpoints, POST /characters/{id}/equip-armor, POST /characters/{id}/equip-weapon, PATCH /currency/modify)**:
  - Clients send intents (e.g., "equip armor", "apply hitpoint delta", "modify currency"), not final state.
  - Server validates ownership and action legality.
  - Server checks version for optimistic concurrency control.
  - On success, server returns the updated character with all derived stats.
  - On version conflict, server returns 409 Conflict with the latest character state and derived stats.

After any successful character modification, the server will return the full updated character state, including base fields and any derived side effects (e.g., Armor Class, movement speed, saving throw bonuses). This ensures the client remains fully synchronized with the server’s authoritative view.            

## 6. Concurrency / Race Condition Handling

### Versioning Strategy

Each character entity maintains a `version` field that is incremented on every successful update. This ensures optimistic concurrency control.

### Example Flows

**Taking Damage:**
1. Client fetches character (HP: 30, Version: 5).
2. Client sends PATCH request to reduce HP by 10, providing version 5.
3. Server checks version:
   - If matches (5), applies change (HP 20), increments version to 6.
   - If mismatch, responds with HTTP 409 Conflict **and returns the latest character state**.

![Versioning](1.b-versioning.png)

### On Version Mismatch

If the client's version does not match the server's, the server responds:
```json
HTTP 409 Conflict
{
  "error": "Version mismatch. Please refresh.",
  "character": {
    "id": "f81d4fae-7dec-11d0-a765-00a0c91e6bf6",
    "hitpoints": 20,
    "version": 6,
  }
}
```
This allows the client to update its local copy. It is up to the application
to decide what to do when a conflict occurs. Attempting to reduce the hitpoints
on top of this new state, may or may not make sense.

### Cache to Database Persistence

**Async Batch Writing:**
- After every successful update, character state is updated in cache and enqueued into an async write queue.
- A background worker periodically (every 1-5 seconds) pulls characters from the queue and writes them in batch to the database.
- Batching reduces database load and improves throughput.

**Deduplication:**
- If multiple updates occur to the same character within a batch window, only the latest version is written to the database.

**Failure Handling:**
- If database writes fail after retries, the character ID can be moved to a dead letter queue for manual intervention.

## 7. Security Measures

- **Bearer token authentication** on all APIs.
- **Ownership checks** between session and requested character.
- **No trust in client-provided data**: Only send actions/commands, never final state.

## 8. System Health Monitoring

| Service Area            | Metric                  | Threshold                                      |
|--------------------------|--------------------------|------------------------------------------------|
| API Gateway              | 5xx error rate           | Alert if >1% in 5 minutes                      |
| Auth Service             | Login success rate       | Alert if <95%                                  |
| Database                 | Write latency            | Alert if >100ms sustained for 5 minutes        |
| Player Management APIs   | Latency (p95)            | Alert if >300ms                               |

### Tools
- **Prometheus + Grafana**: Metrics and alerts.
- **AWS CloudWatch**: Infrastructure monitoring.
- **OpenTelemetry**: Distributed tracing.
- **Health Endpoints**:
  - `/health/live`
  - `/health/ready`
  - `/metrics`

